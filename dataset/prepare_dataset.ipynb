{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "We build a dataset using tfrecords and webp format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "#!pip install img2dataset tensorflow tensorflow_io wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting URL list\n",
    "\n",
    "We use the [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Conceptual Captions\n",
    "!wget https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv -O GCC-valid.tsv\n",
    "!wget https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv -O GCC-train.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We format input files to keep only url's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the files and keep only url\n",
    "for f, name in zip(\n",
    "    [\"GCC-train.tsv\", \"GCC-valid.tsv\"],\n",
    "    [\"train\", \"valid\"],\n",
    "):\n",
    "    df = pd.read_csv(f, sep=\"\\t\", names=[\"caption\", \"url\"])\n",
    "    df.to_parquet(f\"{name}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets may be a bit large so we reduce their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, max_items in [(\"train.parquet\", 500_000), (\"valid.parquet\", 10_000)]:\n",
    "    df = pd.read_parquet(path)\n",
    "    print(f\"{path}: keeping {max_items} / {len(df)}\")\n",
    "    df = df[:max_items]\n",
    "    df.to_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir cc3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for validation set\n",
    "input_file = \"valid.parquet\"\n",
    "output_folder = \"cc3m/valid\"\n",
    "input_format = \"parquet\"\n",
    "caption_col = \"caption\"\n",
    "image_size = 256\n",
    "processes_count = 80\n",
    "thread_count = 16\n",
    "encode_quality = 100\n",
    "encode_format = \"webp\"\n",
    "number_sample_per_shard = 1000\n",
    "min_image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!img2dataset \\\n",
    "  --url_list $input_file \\\n",
    "  --image_size $image_size \\\n",
    "  --output_folder $output_folder \\\n",
    "  --input_format $input_format \\\n",
    "  --caption_col $caption_col \\\n",
    "  --processes_count $processes_count \\\n",
    "  --thread_count $thread_count \\\n",
    "  --resize_mode center_crop \\\n",
    "  --encode_quality $encode_quality \\\n",
    "  --encode_format $encode_format \\\n",
    "  --output_format tfrecord \\\n",
    "  --number_sample_per_shard $number_sample_per_shard \\\n",
    "  --extract_exif false \\\n",
    "  --min_image_size $min_image_size \\\n",
    "  --enable_wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update relevant parameters for train set\n",
    "input_file = \"train.parquet\"\n",
    "output_folder = \"cc3m/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!img2dataset \\\n",
    "  --url_list $input_file \\\n",
    "  --image_size $image_size \\\n",
    "  --output_folder $output_folder \\\n",
    "  --input_format $input_format \\\n",
    "  --caption_col $caption_col \\\n",
    "  --processes_count $processes_count \\\n",
    "  --thread_count $thread_count \\\n",
    "  --resize_mode center_crop \\\n",
    "  --encode_quality $encode_quality \\\n",
    "  --encode_format $encode_format \\\n",
    "  --output_format tfrecord \\\n",
    "  --number_sample_per_shard $number_sample_per_shard \\\n",
    "  --extract_exif false \\\n",
    "  --min_image_size $min_image_size \\\n",
    "  --enable_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files have been saved as tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_jax.data import Dataset, logits_to_image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Calculate mean and std of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the statistics on the validation set to use a center crop instead of random crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    valid_folder=\"cc3m/train\",\n",
    "    valid_batch_size=1000,\n",
    "    image_size=224,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parallelize the calculation of mean and std because we are efficient people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(images, captions):\n",
    "    images = tf.cast(images, tf.float64)\n",
    "    mean = tf.reduce_mean(images, axis=(0, 2, 3))\n",
    "    std = tf.math.reduce_std(images, axis=(0, 2, 3))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean_std = dataset.valid.map(\n",
    "    get_mean_std, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "\n",
    "for batch in tqdm(ds_mean_std):\n",
    "    mean, std = batch\n",
    "    means.append(mean)\n",
    "    stds.append(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the global mean and std\n",
    "mean = tf.stack(means, axis=0)\n",
    "std = tf.stack(stds, axis=0)\n",
    "mean = tf.reduce_mean(mean, axis=0)\n",
    "std = tf.math.sqrt(tf.reduce_sum(tf.math.square(std) / len(stds), axis=0))\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    train_folder=\"cc3m/train\",\n",
    "    valid_folder=\"cc3m/train\",\n",
    "    train_batch_size=10,\n",
    "    valid_batch_size=10,\n",
    "    image_size=224,\n",
    "    mean=[0.5, 0.5, 0.5],\n",
    "    std=[0.5, 0.5, 0.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(dataset.train.as_numpy_iterator()))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sample_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the batch\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    img = logits_to_image(\n",
    "        images[i], mean=dataset.mean, std=dataset.std, format=dataset.format\n",
    "    )\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_REPO = \"openai/clip-vit-large-patch14\"\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(CLIP_REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = sample_batch[1]\n",
    "captions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "captions = [caption.decode(\"utf-8\") for caption in captions]\n",
    "txt_inputs = tokenizer(\n",
    "    captions, padding=\"max_length\", truncation=True, return_tensors=\"np\"\n",
    ")\n",
    "txt_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FlaxCLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlaxCLIPModel.from_pretrained(CLIP_REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"pixel_values\": images, **txt_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a62b92fe2e5fe408a75bd440461aa67101c86b1a853d17394039a983fcc1114"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
